{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your AI Agent with MCPs using SGLang, Pydantic AI, and AMD MI300X GPU\n",
    "\n",
    "Welcome to this hands-on workshop! Throughout this tutorial, we'll leverage AMD GPUs and **Model Context Protocol (MCP)** ,an open standard for exposing LLM tools via API, to deploy powerful language models like Qwen2.5-7B. Key components:\n",
    "- üñ•Ô∏è **SGLang** for GPU-optimized inference\n",
    "- üõ†Ô∏è **Pydantic-AI** for agent/tool management\n",
    "- üîå **MCP Servers** for pre-built tool integration\n",
    "\n",
    "You'll learn how to set up your environment, deploy large language models like Qwen2.5, connect them to real-world tools using MCP, and build a conversational agent capable of reasoning and taking actions.\n",
    "\n",
    "By the end of this workshop, you‚Äôll have built an AI-powered assistant agent that can find a place to stay based on your preferences like location, budget, and travel dates.\n",
    "\n",
    "Let‚Äôs dive in!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Step 1: Launching SGLang Server on AMD GPUs](#step1)\n",
    "- [Step 2: Installing Dependencies](#step2)\n",
    "- [Step 3: Create a simple instance of Pydantic-AI Agent](#step3)\n",
    "- [Step 4: Write a Date/Time Tool for your Agent](#step4)\n",
    "- [Step 5: Replace your Date/Time Tool with a MCP Server](#step5)\n",
    "- [Step 6: Turn your Agent into a trip planner](#step6)\n",
    "- [Step 7: Challenge](#step7)\n",
    "\n",
    "<a id=\"step1\"></a>\n",
    "\n",
    "## Step 1: Launch a SGLang Server\n",
    "\n",
    "In this workshop we are going to use [SGLang](https://github.com/sgl-project/sglang) as our inference serving engine. SGLang provides many benefits such as fast model execution, extensive list of supported models, easy to use, and best of all it's open-source. \n",
    "\n",
    "### Deploy Qwen2.5-7B-Instruct Model with SGLang (~3 mins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, we need to start a SGLang server and create an OpenAI-compatible endpoint for your LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "INFO 08-15 16:03:47 __init__.py:179] Automatically detected platform rocm.\n",
      "WARNING 08-15 16:03:47 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.\n",
      "WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.configurer:Failed to import deep_gemm, disable ENABLE_JIT_DEEPGEMM.\n",
      "/sgl-workspace/hubert_sgl/python/sglang/srt/layers/quantization/awq.py:42: UserWarning: Using kernels directly from vllm. This might lead to performance degradation or missing functionalities as certain kernels may not be optimized. \n",
      "  warnings.warn(\n",
      "/sgl-workspace/hubert_sgl/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.\n",
      "  warnings.warn(f\"HIP does not support fused_marlin_moe currently.\")\n",
      "[2025-08-15 16:03:49] server_args=ServerArgs(model_path='Qwen/Qwen2.5-7B-Instruct', tokenizer_path='Qwen/Qwen2.5-7B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=30221, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.833, max_running_requests=128, max_queued_requests=9223372036854775807, max_total_tokens=20480, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=794502734, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, served_model_name='Qwen/Qwen2.5-7B-Instruct', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser='qwen25', tool_server=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, moe_a2a_backend=None, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=4, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False, scheduler_recv_interval=1, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, enable_ep_moe=False, enable_deepep_moe=False)\n",
      "[2025-08-15 16:03:50] Using default HuggingFace chat template with detected content format: string\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "INFO 08-15 16:03:59 __init__.py:179] Automatically detected platform rocm.\n",
      "INFO 08-15 16:03:59 __init__.py:179] Automatically detected platform rocm.\n",
      "WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.configurer:Failed to import deep_gemm, disable ENABLE_JIT_DEEPGEMM.\n",
      "WARNING:sglang.srt.layers.quantization.deep_gemm_wrapper.configurer:Failed to import deep_gemm, disable ENABLE_JIT_DEEPGEMM.\n",
      "/sgl-workspace/hubert_sgl/python/sglang/srt/layers/quantization/awq.py:42: UserWarning: Using kernels directly from vllm. This might lead to performance degradation or missing functionalities as certain kernels may not be optimized. \n",
      "  warnings.warn(\n",
      "/sgl-workspace/hubert_sgl/python/sglang/srt/layers/quantization/awq.py:42: UserWarning: Using kernels directly from vllm. This might lead to performance degradation or missing functionalities as certain kernels may not be optimized. \n",
      "  warnings.warn(\n",
      "/sgl-workspace/hubert_sgl/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.\n",
      "  warnings.warn(f\"HIP does not support fused_marlin_moe currently.\")\n",
      "/sgl-workspace/hubert_sgl/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.\n",
      "  warnings.warn(f\"HIP does not support fused_marlin_moe currently.\")\n",
      "[2025-08-15 16:04:00] Process 2931 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]\n",
      "[2025-08-15 16:04:00] Attention backend not explicitly specified. Use aiter backend by default.\n",
      "[2025-08-15 16:04:00] Init torch distributed begin.\n",
      "[2025-08-15 16:04:00] Init torch distributed ends. mem usage=0.00 GB\n",
      "[2025-08-15 16:04:01] Ignore import error when loading sglang.srt.models.glm4v_moe: No module named 'transformers.models.glm4v_moe'\n",
      "[2025-08-15 16:04:01] Load weight begin. avail mem=191.47 GB\n",
      "[2025-08-15 16:04:01] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.44s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.54s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.52s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.53s/it]\n",
      "\n",
      "[2025-08-15 16:04:08] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=177.01 GB, mem usage=14.46 GB.\n",
      "[2025-08-15 16:04:08] KV Cache is allocated. #tokens: 20480, K size: 0.55 GB, V size: 0.55 GB\n",
      "[2025-08-15 16:04:08] Memory pool end. avail mem=175.79 GB\n",
      "[2025-08-15 16:04:11] Capture cuda graph begin. This can take up to several minutes. avail mem=175.36 GB\n",
      "[2025-08-15 16:04:11] Capture cuda graph bs [1, 2, 4]\n",
      "Capturing batches (bs=1 avail_mem=174.91 GB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.14s/it]\n",
      "[2025-08-15 16:04:14] Capture cuda graph end. Time elapsed: 3.80 s. mem usage=0.46 GB. avail mem=174.91 GB.\n",
      "[2025-08-15 16:04:15] max_total_num_tokens=20480, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=128, context_len=32768, available_gpu_mem=174.91 GB\n",
      "[2025-08-15 16:04:15] INFO:     Started server process [2592]\n",
      "[2025-08-15 16:04:15] INFO:     Waiting for application startup.\n",
      "[2025-08-15 16:04:15] INFO:     Application startup complete.\n",
      "[2025-08-15 16:04:15] INFO:     Uvicorn running on http://0.0.0.0:30221 (Press CTRL+C to quit)\n",
      "[2025-08-15 16:04:16] INFO:     127.0.0.1:56670 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "[2025-08-15 16:04:16] INFO:     127.0.0.1:56686 - \"GET /get_model_info HTTP/1.1\" 200 OK\n",
      "[2025-08-15 16:04:16] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]\n",
      "[2025-08-15 16:04:17] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]\n",
      "[2025-08-15 16:04:18] INFO:     127.0.0.1:56692 - \"POST /generate HTTP/1.1\" 200 OK\n",
      "[2025-08-15 16:04:18] The server is fired up and ready to roll!\n",
      "\n",
      "\n",
      "                    NOTE: Typically, the server runs in a separate terminal.\n",
      "                    In this notebook, we run the server and notebook code together, so their outputs are combined.\n",
      "                    To improve clarity, the server logs are displayed in the original black color, while the notebook outputs are highlighted in blue.\n",
      "                    We are running those notebooks in a CI environment, so the throughput is not representative of the actual performance.\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sglang.test.doc_patch import launch_server_cmd\n",
    "from sglang.utils import wait_for_server\n",
    "\n",
    "os.environ[\"SGLANG_USE_AITER\"] = \"1\"\n",
    "_server_process, _port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --tool-call-parser qwen25 --host 0.0.0.0 --port 8000\"  # qwen25\n",
    ")\n",
    "wait_for_server(f\"http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open a terminal and watch the GPU utilization by running this command:\n",
    "\n",
    "```bash\n",
    "watch rocm-smi\n",
    "```\n",
    "\n",
    "Let's set some environment variables for our server to use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set: http://localhost:30221/v1\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = f\"http://localhost:8000/v1\"\n",
    "\n",
    "os.environ[\"BASE_URL\"]    = BASE_URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"abc-123\"   \n",
    "\n",
    "print(\"Config set:\", BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify your model is available at the `BASE_URL` we just set by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (3) URL using bad/illegal format or missing URL\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8000/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now just launched a powerful server that can serve any incoming request and allowing you to build amazing applications. Wasn't that easy?üéâ \n",
    "\n",
    "<a id=\"step2\"></a>\n",
    "\n",
    "## Step 2: Installing Dependencies\n",
    "\n",
    "We are going to use `Pydantic AI`. Let's install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: deps present.\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "need = []\n",
    "for mod in (\"mcp\", \"httpx\", \"duckduckgo_search\", \"pydantic_ai\"):\n",
    "    try:\n",
    "        __import__(mod if mod != \"duckduckgo_search\" else \"duckduckgo_search\")\n",
    "    except Exception:\n",
    "        need.append(mod)\n",
    "\n",
    "if need:\n",
    "    pip_install(need)\n",
    "\n",
    "print(\"OK: deps present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"step3\"></a>\n",
    "\n",
    "## Step 3: Create a simple instance of Pydantic-AI Agent\n",
    "\n",
    "Let's start by creating a custom OpenAI Compatible endpoint for our agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "provider = OpenAIProvider(\n",
    "    base_url=os.environ[\"BASE_URL\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "agent_model = OpenAIModel(\"Qwen2.5-7B-Instruct\", provider=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating an instance the `Agent` class from `pydantic_ai`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=agent_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to test the agent. `pydantic_ai` provides multiple ways to run `Agent`. You can learn more about it [here](https://ai.pydantic.dev/agents/#running-agents).\n",
    "\n",
    "In this workshop, we are running in `async` mode. We are going to define a helper function that allows us to quickly test our agent throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "async def run_async(prompt: str) -> str:\n",
    "    async with agent:\n",
    "        result = await agent.run(prompt)\n",
    "        return result.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the agent by calling this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 16:04:22] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 16:04:22] INFO:     127.0.0.1:43958 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_async(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now that we have the basics of creating an agent instance, and connecting it to the model we started serving with vLLM earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "\n",
    "## Step 4: Write a Date/Time Tool for Your Agent\n",
    "\n",
    "LLMs naturally rely on their training data to respond to your prompts. Therefore, the agent we just defined fails to answer a factual question that falls outside of it's training knowledge. Let's show this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 16:04:22] Prefill batch. #new-seq: 1, #new-token: 10, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 16:04:22] Decode batch. #running-req: 1, #token: 60, token usage: 0.00, cuda graph: True, gen throughput (token/s): 5.61, #queue-req: 0, \n",
      "[2025-08-15 16:04:22] INFO:     127.0.0.1:43958 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have real-time capabilities and I don't have access to current dates. If you need to know today's date, I recommend checking a calendar or an online source for the most accurate information.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that the model failed to answer this question. Now, it's time to power-up your LLM by providing `agent` a function that can get the current date. The process of an LLM triggering a function call is commonly referred to as `Tool Calling` or `Function Calling`. In this workshop we are going to take advantage of `pydantic-ai`'s agent `tools` to provide our agent appropriate tools. First, we need to define a custom tool. Below is how we can define a tool in this framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pydantic_ai import Tool          \n",
    "@Tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Return the current date/time as an ISO-formatted string.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to provide this tool to our Agent, as this will notify the LLM about the existence of such a tool we just definied. This is simply done by just providing the function signiture of the tool we just defined to our agent constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    tools=[get_current_date],\n",
    "    system_prompt = (\n",
    "        \"You have access to:\\n\"\n",
    "        \"   1. get_current_time(params: dict)\\n\"\n",
    "        \"Use this tool for date/time questions.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 16:04:22] Prefill batch. #new-seq: 1, #new-token: 164, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 16:04:22] INFO:     127.0.0.1:43958 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-15 16:04:22] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 181, token usage: 0.01, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 16:04:22] Decode batch. #running-req: 1, #token: 224, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.87, #queue-req: 0, \n",
      "[2025-08-15 16:04:22] INFO:     127.0.0.1:43958 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Today's date is 2025-08-15.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done on building an agent with access to real-time data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"step5\"></a>\n",
    "\n",
    "## Step 5: Replace Your Date/time Tool with a MCP server\n",
    "\n",
    "Now that we learned how to create a custom tool and provide the agent access to this tool. Let's now explore a trendy topic of [Model Context Protocol](https://modelcontextprotocol.io/introduction). We are going to explore how we can replace our custom tool with a simple MCP server that can serve our agent and provide similar information.\n",
    "\n",
    "**Why MCP?** MCP servers provide:\n",
    "- ‚úÖ Standardized API interfaces\n",
    "- üîÑ Reusable across projects\n",
    "- üì¶ Pre-built functionality\n",
    "\n",
    "Let's replace our custom time tool with an official MCP time server:\n",
    "\n",
    "### Installing Time MCP Server\n",
    "\n",
    "We are going to start by installing this MCP server:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q mcp-server-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our time_server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "\n",
    "time_server = MCPServerStdio(\n",
    "    \"python\",\n",
    "    args=[\n",
    "        \"-m\", \"mcp_server_time\",\n",
    "        \"--local-timezone=America/New_York\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's modify our agent to remove our previously defined tool, and add this MCP server instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    mcp_servers=[time_server],\n",
    "    system_prompt = (\n",
    "        \"You are a helpful agent and you have access to this tool:\\n\"\n",
    "        \"   get_current_time(params: dict)\\n\"\n",
    "        \"When the user asks for the current date or time, call get_current_time.\\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Great, let's see if the agent can use the MCP to give us the correct time now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-15 16:06:11] Prefill batch. #new-seq: 1, #new-token: 423, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 16:06:11] INFO:     127.0.0.1:41416 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-15 16:06:11] Decode batch. #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.37, #queue-req: 0, \n",
      "[2025-08-15 16:06:11] Prefill batch. #new-seq: 1, #new-token: 79, #cached-token: 433, token usage: 0.02, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 16:06:11] INFO:     127.0.0.1:41416 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Today's date in New York is August 15, 2025.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Turn your agents for travel planning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;79m2025-08-15 16:06:17 - Installing pre-requisites\u001b[0m\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \n",
      "Hit:5 https://repo.radeon.com/amdgpu/6.3/ubuntu jammy InRelease                \n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3521 kB]\n",
      "Hit:7 https://repo.radeon.com/rocm/apt/6.3 jammy InRelease                     \n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1575 kB]\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
      "Fetched 5531 kB in 1s (5537 kB/s)    \n",
      "Reading package lists... Done\n",
      "W: https://repo.radeon.com/amdgpu/6.3/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "W: https://repo.radeon.com/rocm/apt/6.3/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ca-certificates is already the newest version (20240203~22.04.1).\n",
      "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  composablekernel-dev half hipcub-dev rccl rccl-dev\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  dirmngr gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client gpg-wks-server\n",
      "  gpgconf gpgsm gpgv\n",
      "Suggested packages:\n",
      "  dbus-user-session pinentry-gnome3 tor parcimonie xloadimage scdaemon\n",
      "The following NEW packages will be installed:\n",
      "  apt-transport-https\n",
      "The following packages will be upgraded:\n",
      "  dirmngr gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm gpgv\n",
      "11 upgraded, 1 newly installed, 0 to remove and 102 not upgraded.\n",
      "Need to get 2249 kB of archives.\n",
      "After this operation, 170 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-client amd64 2.2.27-3ubuntu2.4 [62.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dirmngr amd64 2.2.27-3ubuntu2.4 [293 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-wks-server amd64 2.2.27-3ubuntu2.4 [57.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-utils amd64 2.2.27-3ubuntu2.4 [309 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg-agent amd64 2.2.27-3ubuntu2.4 [209 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpg amd64 2.2.27-3ubuntu2.4 [518 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgconf amd64 2.2.27-3ubuntu2.4 [94.5 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg-l10n all 2.2.27-3ubuntu2.4 [54.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnupg all 2.2.27-3ubuntu2.4 [315 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgsm amd64 2.2.27-3ubuntu2.4 [197 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gpgv amd64 2.2.27-3ubuntu2.4 [137 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 apt-transport-https all 2.4.14 [1510 B]\n",
      "Fetched 2249 kB in 1s (1722 kB/s)              \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 51871 files and directories currently installed.)\n",
      "Preparing to unpack .../00-gpg-wks-client_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../01-dirmngr_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../02-gpg-wks-server_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../03-gnupg-utils_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../04-gpg-agent_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../05-gpg_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpg (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../06-gpgconf_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../07-gnupg-l10n_2.2.27-3ubuntu2.4_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../08-gnupg_2.2.27-3ubuntu2.4_all.deb ...\n",
      "Unpacking gnupg (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../09-gpgsm_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Preparing to unpack .../10-gpgv_2.2.27-3ubuntu2.4_amd64.deb ...\n",
      "Unpacking gpgv (2.2.27-3ubuntu2.4) over (2.2.27-3ubuntu2.1) ...\n",
      "Setting up gpgv (2.2.27-3ubuntu2.4) ...\n",
      "Selecting previously unselected package apt-transport-https.\n",
      "(Reading database ... 51871 files and directories currently installed.)\n",
      "Preparing to unpack .../apt-transport-https_2.4.14_all.deb ...\n",
      "Unpacking apt-transport-https (2.4.14) ...\n",
      "Setting up apt-transport-https (2.4.14) ...\n",
      "Setting up gnupg-l10n (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gpgconf (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gpg (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gnupg-utils (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gpg-agent (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gpgsm (2.2.27-3ubuntu2.4) ...\n",
      "Setting up dirmngr (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gpg-wks-server (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gpg-wks-client (2.2.27-3ubuntu2.4) ...\n",
      "Setting up gnupg (2.2.27-3ubuntu2.4) ...\n",
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Get:2 https://deb.nodesource.com/node_20.x nodistro InRelease [12.1 kB]        \n",
      "Hit:3 https://repo.radeon.com/amdgpu/6.3/ubuntu jammy InRelease                \n",
      "Get:4 https://deb.nodesource.com/node_20.x nodistro/main amd64 Packages [12.4 kB]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Hit:6 https://repo.radeon.com/rocm/apt/6.3 jammy InRelease                     \n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease      \n",
      "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Fetched 24.6 kB in 1s (38.5 kB/s)\n",
      "Reading package lists... Done\n",
      "W: https://repo.radeon.com/amdgpu/6.3/ubuntu/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "W: https://repo.radeon.com/rocm/apt/6.3/dists/jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "\u001b[1;34m2025-08-15 16:06:24 - Repository configured successfully.\u001b[0m\n",
      "\u001b[38;5;79m2025-08-15 16:06:24 - To install Node.js, run: apt-get install nodejs -y\u001b[0m\n",
      "\u001b[38;5;79m2025-08-15 16:06:24 - You can use N|solid Runtime as a node.js alternative\u001b[0m\n",
      "\u001b[1;32m2025-08-15 16:06:24 - To install N|solid Runtime, run: apt-get install nsolid -y \n",
      "\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  composablekernel-dev half hipcub-dev rccl rccl-dev\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following NEW packages will be installed:\n",
      "  nodejs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 102 not upgraded.\n",
      "Need to get 32.0 MB of archives.\n",
      "After this operation, 199 MB of additional disk space will be used.\n",
      "Get:1 https://deb.nodesource.com/node_20.x nodistro/main amd64 nodejs amd64 20.19.4-1nodesource1 [32.0 MB]\n",
      "Fetched 32.0 MB in 2s (13.2 MB/s) \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package nodejs.\n",
      "(Reading database ... 51875 files and directories currently installed.)\n",
      "Preparing to unpack .../nodejs_20.19.4-1nodesource1_amd64.deb ...\n",
      "Unpacking nodejs (20.19.4-1nodesource1) ...\n",
      "Setting up nodejs (20.19.4-1nodesource1) ...\n",
      "v20.19.4\n",
      "10.8.2\n",
      "\u001b[1G\u001b[0K10.8.2\n",
      "\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\n",
    "!sudo apt-get install -y nodejs\n",
    "!node -v && npm -v && npx -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tadaa! Now you have officially used MCP servers to power-up your AI agents. In the next section we show how you can your turn many ideas into real working projects by using free MCP servers available today.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"step6\"></a>\n",
    "\n",
    "## Step 6: Turn your agent to a travel planner\n",
    "\n",
    "As we experience in the last section, MCP servers are really easy to use and they provide a standard way of providing LLMs the tools we need. There are already thousands of MCP servers available for us to use. There are some MCP trackers that you can always use to find out about available servers. Here are some for your reference:\n",
    "- https://github.com/modelcontextprotocol/servers\n",
    "- https://mcp.so/\n",
    "\n",
    "We are going to use npx to launch out next server. Therefore, let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Node.js 20 via NodeSource\n",
    "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\n",
    "!apt install -y nodejs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify `npm` and `npx` installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!node -v && npm -v && npx --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_786/1479450536.py:26: DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n",
      "  if not pkgutil.find_loader(mod):\n"
     ]
    }
   ],
   "source": [
    "import sys, os, subprocess, pkgutil, pathlib, textwrap, asyncio, re, json, math\n",
    "from typing import Any, Dict, List, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 0) Basics + ensure deps\n",
    "PY = sys.executable\n",
    "ROOT = pathlib.Path.cwd() / \"mcp_nb\"\n",
    "ROOT.mkdir(exist_ok=True)\n",
    "BASE_ENV = os.environ.copy()\n",
    "BASE_ENV[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "\n",
    "def ensure_pkgs():\n",
    "    to_check = [\n",
    "        (\"mcp\", \"mcp\"),\n",
    "        (\"pydantic_ai\", \"pydantic-ai\"),\n",
    "        (\"duckduckgo_search\", \"duckduckgo-search\"),\n",
    "        (\"httpx\", \"httpx\"),\n",
    "        (\"tzdata\", \"tzdata\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pipname in to_check:\n",
    "        if not any(m == mod or m.startswith(mod + \".\") for m in sys.modules):\n",
    "            if not pkgutil.find_loader(mod):\n",
    "                missing.append(pipname)\n",
    "    if missing:\n",
    "        print(\"Installing:\", \", \".join(sorted(set(missing))))\n",
    "        subprocess.check_call([PY, \"-m\", \"pip\", \"install\", \"-q\", *sorted(set(missing))])\n",
    "\n",
    "ensure_pkgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote servers to /sgl-workspace/hubert_sgl/mcp_nb\n",
      "‚úó time disabled: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "‚úó search disabled: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "üöÄ Running demo...\n",
      "üß≠ Planner‚Ä¶\n",
      "[2025-08-15 18:59:50] INFO:     127.0.0.1:56394 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-15 18:59:50] Prefill batch. #new-seq: 1, #new-token: 56, #cached-token: 344, token usage: 0.02, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 18:59:50] Decode batch. #running-req: 1, #token: 419, token usage: 0.02, cuda graph: True, gen throughput (token/s): 0.14, #queue-req: 0, \n",
      "[2025-08-15 18:59:50] Decode batch. #running-req: 1, #token: 459, token usage: 0.02, cuda graph: True, gen throughput (token/s): 186.93, #queue-req: 0, \n",
      "[2025-08-15 18:59:50] Decode batch. #running-req: 1, #token: 499, token usage: 0.02, cuda graph: True, gen throughput (token/s): 186.63, #queue-req: 0, \n",
      "[2025-08-15 18:59:50] Decode batch. #running-req: 1, #token: 539, token usage: 0.03, cuda graph: True, gen throughput (token/s): 186.36, #queue-req: 0, \n",
      "[2025-08-15 18:59:51] Decode batch. #running-req: 1, #token: 579, token usage: 0.03, cuda graph: True, gen throughput (token/s): 186.13, #queue-req: 0, \n",
      "[2025-08-15 18:59:51] Decode batch. #running-req: 1, #token: 619, token usage: 0.03, cuda graph: True, gen throughput (token/s): 185.95, #queue-req: 0, \n",
      "[2025-08-15 18:59:51] Decode batch. #running-req: 1, #token: 659, token usage: 0.03, cuda graph: True, gen throughput (token/s): 185.91, #queue-req: 0, \n",
      "[2025-08-15 18:59:51] Decode batch. #running-req: 1, #token: 699, token usage: 0.03, cuda graph: True, gen throughput (token/s): 185.74, #queue-req: 0, \n",
      "[2025-08-15 18:59:51] Decode batch. #running-req: 1, #token: 739, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.66, #queue-req: 0, \n",
      "[2025-08-15 18:59:52] Decode batch. #running-req: 1, #token: 779, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.52, #queue-req: 0, \n",
      "[2025-08-15 18:59:52] Decode batch. #running-req: 1, #token: 819, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.44, #queue-req: 0, \n",
      "‚õÖ Weather‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Research‚Ä¶\n",
      "[2025-08-15 18:59:54] INFO:     127.0.0.1:56394 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-15 18:59:54] Prefill batch. #new-seq: 1, #new-token: 162, #cached-token: 397, token usage: 0.02, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 18:59:54] Decode batch. #running-req: 1, #token: 573, token usage: 0.03, cuda graph: True, gen throughput (token/s): 16.87, #queue-req: 0, \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## ‚úàÔ∏è Flights\n",
       "| # | Title | Price | Notes |\n",
       "|---:|---|---:|---|\n",
       "| 1 | [Google Flights](https://www.google.com/travel/flights/search?q=Flights%20to%20Narita%20from%20San%20Francisco%20on%202025-08-22%20through%202025-08-31) | $986 | Estimated roundtrip (distance √ó CPM); click for live fares |\n",
       "\n",
       "\n",
       "## üè® Hotels\n",
       "| # | Title | Price | Notes |\n",
       "|---:|---|---:|---|\n",
       "| 1 | [Booking.com](https://www.booking.com/searchresults.html?ss=Narita&checkin=2025-08-22&checkout=2025-08-31) | $1,260 | Estimated total for 9 night(s) (‚âà $140/night) |\n",
       "\n",
       "\n",
       "## üìÖ Day-by-day\n",
       "| Date | Plan |\n",
       "|---|---|\n",
       "| 2025-08-22 | Arrival and accommodation setup<br>‚Ä¢ Arrive at Narita Airport<br>‚Ä¢ Check into hotel<br>‚Ä¢ Set up accommodation<br>‚Ä¢ Wx: 25‚Äì34¬∞C, 0 mm. Light layers. |\n",
       "| 2025-08-23 | Exploring Tokyo's Manga Culture<br>‚Ä¢ Visit Akihabara for Manga and Anime<br>‚Ä¢ Explore Tokyo Metropolitan Government Building<br>‚Ä¢ Wx: 26‚Äì34¬∞C, 0 mm. Light layers. |\n",
       "| 2025-08-24 | Experience Traditional Japanese Culture<br>‚Ä¢ Visit Tokyo National Museum<br>‚Ä¢ Enjoy a traditional tea ceremony<br>‚Ä¢ Wx: 22‚Äì31¬∞C, 5 mm. Light layers. Chance of rain ‚Äî bring an umbrella. |\n",
       "| 2025-08-25 | Sushi and Japanese Cuisine<br>‚Ä¢ Visit Tsukiji Outer Market for fresh sushi<br>‚Ä¢ Dinner at a local izakaya<br>‚Ä¢ Wx: 21‚Äì23¬∞C, 4 mm. Light layers. Chance of rain ‚Äî bring an umbrella. |\n",
       "| 2025-08-26 | Historical Buildings and Temples<br>‚Ä¢ Visit Senso-ji Temple<br>‚Ä¢ Explore Asakusa district<br>‚Ä¢ Wx: 21‚Äì27¬∞C, 0 mm. Light layers. |\n",
       "| 2025-08-27 | Shopping and Relaxation<br>‚Ä¢ Visit Shibuya and Harajuku<br>‚Ä¢ Relax at a local onsen<br>‚Ä¢ Wx: 22‚Äì29¬∞C, 0 mm. Light layers. |\n",
       "| 2025-08-28 | Explore Tokyo‚Äôs Modern Side<br>‚Ä¢ Visit Shinjuku Gyoen National Garden<br>‚Ä¢ Shopping in Omotesando<br>‚Ä¢ Wx: 25‚Äì34¬∞C, 0 mm. Light layers. |\n",
       "| 2025-08-29 | Day Trip to Yokohama<br>‚Ä¢ Visit Yokohama Chinatown<br>‚Ä¢ Yokohama Red Brick Warehouse<br>‚Ä¢ Wx: 24‚Äì27¬∞C, 1 mm. Light layers. |\n",
       "| 2025-08-30 | Last Day Explorations<br>‚Ä¢ Visit Tokyo Skytree<br>‚Ä¢ Explore Odaiba area<br>‚Ä¢ Wx: 22‚Äì25¬∞C, 3 mm. Light layers. |\n",
       "| 2025-08-31 | Departure from Narita Airport<br>‚Ä¢ Pack and check out of hotel<br>‚Ä¢ Departure from Narita Airport<br>‚Ä¢ Wx: 18‚Äì24¬∞C, 1 mm. Light layers. |\n",
       "\n",
       "\n",
       "## üöâ Local transit\n",
       "Consider local transit passes in Taipei. Compare airport rail vs rideshare.\n",
       "\n",
       "\n",
       "## üíµ Cost summary\n",
       "| Item | Cost (USD) |\n",
       "|---|---:|\n",
       "| Flights | 986 |\n",
       "| Hotels | 1,260 |\n",
       "| Activities | 350 |\n",
       "| Transit | 120 |\n",
       "| **Total** | **2,716** |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os, pkgutil, subprocess, pathlib, textwrap, asyncio, re, json, math\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# 0) Basics + ensure deps\n",
    "PY = sys.executable\n",
    "ROOT = pathlib.Path.cwd() / \"mcp_nb\"\n",
    "ROOT.mkdir(exist_ok=True)\n",
    "BASE_ENV = os.environ.copy()\n",
    "BASE_ENV[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "\n",
    "def ensure_pkgs():\n",
    "    to_check = [\n",
    "        (\"mcp\", \"mcp\"),\n",
    "        (\"pydantic_ai\", \"pydantic-ai\"),\n",
    "        (\"duckduckgo_search\", \"duckduckgo-search\"),\n",
    "        (\"httpx\", \"httpx\"),\n",
    "        (\"tzdata\", \"tzdata\"),\n",
    "        (\"openai\", \"openai\"),          # for OpenAIModel\n",
    "        (\"anthropic\", \"anthropic\"),    # optional AnthropicModel\n",
    "        (\"nest_asyncio\", \"nest-asyncio\"),\n",
    "    ]\n",
    "    missing = []\n",
    "    for mod, pipname in to_check:\n",
    "        if not any(m == mod or m.startswith(mod + \".\") for m in sys.modules):\n",
    "            if not pkgutil.find_loader(mod):\n",
    "                missing.append(pipname)\n",
    "    if missing:\n",
    "        print(\"Installing:\", \", \".join(sorted(set(missing))))\n",
    "        subprocess.check_call([PY, \"-m\", \"pip\", \"install\", \"-q\", *sorted(set(missing))])\n",
    "\n",
    "ensure_pkgs()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 1) Local MCP servers (Search / Time)\n",
    "search_mcp_code = '''\\\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"SearchMCP\")\n",
    "\n",
    "try:\n",
    "    from duckduckgo_search import DDGS\n",
    "except Exception:\n",
    "    DDGS = None\n",
    "\n",
    "@mcp.tool()\n",
    "def web_search(query: str, max_results: int = 5) -> List[Dict]:\n",
    "    \"\"\"DuckDuckGo web search. Returns [{title, href, snippet}]\"\"\"\n",
    "    out = []\n",
    "    if DDGS is None:\n",
    "        return [{\"title\":\"[search unavailable]\",\"href\":\"\",\"snippet\":\"duckduckgo_search not installed or failed to import.\"}]\n",
    "    try:\n",
    "        with DDGS() as ddg:\n",
    "            for r in ddg.text(query, max_results=max(1, min(int(max_results), 10))):\n",
    "                out.append({\"title\": r.get(\"title\",\"\"), \"href\": r.get(\"href\",\"\"), \"snippet\": r.get(\"body\",\"\")})\n",
    "    except Exception as e:\n",
    "        out.append({\"title\":\"[search error]\", \"href\":\"\", \"snippet\":f\"{type(e).__name__}: {e}\"})\n",
    "    return out\n",
    "\n",
    "@mcp.tool()\n",
    "async def fetch(url: str, max_chars: int = 2000) -> Dict:\n",
    "    \"\"\"Fetch a URL and return {url, status, text[:max_chars]}\"\"\"\n",
    "    import httpx, re\n",
    "    try:\n",
    "        async with httpx.AsyncClient(follow_redirects=True, timeout=20) as client:\n",
    "            resp = await client.get(url)\n",
    "            text = re.sub(r\"\\\\s+\", \" \", resp.text)\n",
    "            return {\"url\": str(resp.url), \"status\": resp.status_code, \"text\": text[:max_chars]}\n",
    "    except Exception as e:\n",
    "        return {\"url\": url, \"status\": 0, \"text\": f\"{type(e).__name__}: {e}\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(\"stdio\")\n",
    "'''\n",
    "\n",
    "time_mcp_code = '''\\\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo\n",
    "except Exception:\n",
    "    ZoneInfo = None\n",
    "\n",
    "mcp = FastMCP(\"TimeMCP\")\n",
    "\n",
    "@mcp.tool()\n",
    "def now(tz: str = \"UTC\") -> Dict[str, Any]:\n",
    "    try:\n",
    "        z = ZoneInfo(tz) if ZoneInfo else None\n",
    "    except Exception:\n",
    "        z = None\n",
    "    if z is None:\n",
    "        from datetime import timezone\n",
    "        z = timezone.utc\n",
    "        tz = \"UTC\"\n",
    "    dt = datetime.now(z)\n",
    "    return {\"iso\": dt.isoformat(), \"date\": dt.strftime(\"%Y-%m-%d\"), \"time\": dt.strftime(\"%H:%M:%S\"), \"tz\": str(z)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(\"stdio\")\n",
    "'''\n",
    "\n",
    "ROOT.joinpath(\"search_mcp.py\").write_text(textwrap.dedent(search_mcp_code), encoding=\"utf-8\")\n",
    "ROOT.joinpath(\"time_mcp.py\").write_text(textwrap.dedent(time_mcp_code), encoding=\"utf-8\")\n",
    "print(\"Wrote servers to\", ROOT)\n",
    "\n",
    "# 2) Launch MCP servers\n",
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "\n",
    "def mk_stdio(cmd, args, name):\n",
    "    return MCPServerStdio(\n",
    "        cmd, args=args,\n",
    "        env=BASE_ENV, cwd=str(ROOT),\n",
    "        timeout=180,\n",
    "        log_level=\"warn\",\n",
    "        log_handler=lambda e: None,   # set to print(...) for verbose logs\n",
    "    )\n",
    "\n",
    "time_server    = mk_stdio(sys.executable, [str(ROOT / \"time_mcp.py\")], \"time\")\n",
    "search_server  = mk_stdio(sys.executable, [str(ROOT / \"search_mcp.py\")], \"search\")\n",
    "\n",
    "async def probe(name, server):\n",
    "    try:\n",
    "        tools = await server.list_tools()\n",
    "        print(f\"‚úì {name} ready: {', '.join(t.name for t in tools)}\")\n",
    "        return server\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {name} disabled: {e}\")\n",
    "        return None\n",
    "\n",
    "servers = asyncio.run(asyncio.gather(\n",
    "    probe(\"time\", time_server),\n",
    "    probe(\"search\", search_server),\n",
    "))\n",
    "servers = [s for s in servers if s]\n",
    "\n",
    "# 3) Models & helpers\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "class Link(BaseModel):\n",
    "    title: str\n",
    "    url: str = \"\"\n",
    "    price: Optional[float] = None\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "class DayPlan(BaseModel):\n",
    "    date: str\n",
    "    summary: str = \"\"\n",
    "    activities: List[str] = Field(default_factory=list)\n",
    "\n",
    "class CostSummary(BaseModel):\n",
    "    currency: str = \"USD\"\n",
    "    flights: float = 0.0\n",
    "    hotels: float = 0.0\n",
    "    activities: float = 0.0\n",
    "    transit: float = 0.0\n",
    "    total: float = 0.0\n",
    "\n",
    "class TripPlan(BaseModel):\n",
    "    flights: List[Link] = Field(default_factory=list)\n",
    "    hotels: List[Link] = Field(default_factory=list)\n",
    "    day_by_day: List[DayPlan] = Field(default_factory=list)\n",
    "    transit_notes: str = \"\"\n",
    "    links: List[Link] = Field(default_factory=list)\n",
    "    cost_summary: CostSummary = Field(default_factory=CostSummary)\n",
    "\n",
    "class PlanSkeleton(BaseModel):\n",
    "    destination_city: str\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    day_by_day: List[DayPlan] = Field(default_factory=list)\n",
    "    notes: str = \"\"\n",
    "\n",
    "class DayIdeas(BaseModel):\n",
    "    date: str\n",
    "    ideas: List[Link] = Field(default_factory=list)\n",
    "\n",
    "class ResearchOutput(BaseModel):\n",
    "    days: List[DayIdeas] = Field(default_factory=list)\n",
    "    links: List[Link] = Field(default_factory=list)\n",
    "\n",
    "# 4) LLM provider pick\n",
    "MODEL = OpenAIModel(\"Qwen2.5-7B-Instruct\", provider=provider)\n",
    "\n",
    "# 5) Utilities (dates, links)\n",
    "def _valid_ymd(s: Optional[str]) -> bool:\n",
    "    if not isinstance(s, str): return False\n",
    "    try:\n",
    "        datetime.strptime(s.strip(), \"%Y-%m-%d\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def daterange(start_date: str, end_date: str) -> List[str]:\n",
    "    s = datetime.strptime(start_date, \"%Y-%m-%d\"); e = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    out, d = [], s\n",
    "    while d <= e:\n",
    "        out.append(d.strftime(\"%Y-%m-%d\")); d += timedelta(days=1)\n",
    "    return out\n",
    "\n",
    "from urllib.parse import quote\n",
    "def google_flights_link(origin: str, dest: str, depart: str, ret: str) -> str:\n",
    "    q = f\"Flights to {dest} from {origin} on {depart} through {ret}\"\n",
    "    return \"https://www.google.com/travel/flights/search?q=\" + quote(q)\n",
    "\n",
    "def booking_link(city: str, start: str, end: str) -> str:\n",
    "    return f\"https://www.booking.com/searchresults.html?ss={quote(city)}&checkin={start}&checkout={end}\"\n",
    "\n",
    "# 6) Dynamic place resolution (city or IATA)\n",
    "async def geocode_full(place: str) -> Optional[Dict[str, Any]]:\n",
    "    import httpx\n",
    "    async with httpx.AsyncClient(timeout=25) as client:\n",
    "        r = await client.get(\"https://geocoding-api.open-meteo.com/v1/search\", params={\"name\": place, \"count\": 1})\n",
    "        data = r.json()\n",
    "        res = data.get(\"results\") or []\n",
    "        if not res: return None\n",
    "        g = res[0]\n",
    "        return {\"name\": g.get(\"name\") or place, \"country\": g.get(\"country\",\"\"), \"lat\": g[\"latitude\"], \"lon\": g[\"longitude\"]}\n",
    "\n",
    "def looks_like_iata(s: str) -> bool:\n",
    "    return isinstance(s, str) and len(s) == 3 and s.isalpha() and s.upper() == s\n",
    "\n",
    "def _pick_city_from_text(text: str) -> Optional[str]:\n",
    "    for pat in [\n",
    "        r\"serv(?:es|ing)\\s+([A-Z][\\w\\- ]{2,40})\",\n",
    "        r\"near\\s+([A-Z][\\w\\- ]{2,40})\",\n",
    "        r\"([A-Z][\\w\\- ]{2,40})\\s+International Airport\",\n",
    "        r\"in\\s+([A-Z][\\w\\- ]{2,40})\\s*(?:,|\\.)\",\n",
    "    ]:\n",
    "        m = re.search(pat, text)\n",
    "        if m:\n",
    "            cand = re.sub(r\"[-‚Äì‚Äî|].*$\", \"\", m.group(1)).strip()\n",
    "            if 2 <= len(cand) <= 48: return cand\n",
    "    return None\n",
    "\n",
    "async def resolve_place(place: str) -> Tuple[str, Optional[Dict[str, Any]]]:\n",
    "    g = await geocode_full(place)\n",
    "    if g: return g[\"name\"], g\n",
    "    if looks_like_iata(place):\n",
    "        try:\n",
    "            from duckduckgo_search import DDGS\n",
    "            with DDGS() as ddg:\n",
    "                rs = list(ddg.text(f\"{place} IATA airport city\", max_results=6))\n",
    "        except Exception:\n",
    "            rs = []\n",
    "        cands = []\n",
    "        for r in rs:\n",
    "            blob = f\"{r.get('title','')} ‚Äî {r.get('body','')}\"\n",
    "            cand = _pick_city_from_text(blob)\n",
    "            if cand: cands.append(cand)\n",
    "        for cand in cands:\n",
    "            g2 = await geocode_full(cand)\n",
    "            if g2: return g2[\"name\"], g2\n",
    "    return place, None\n",
    "\n",
    "# 7) Agents (Planner & Researcher)\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "PLANNER = Agent[None, PlanSkeleton](\n",
    "    model=MODEL,\n",
    "    mcp_servers=servers,\n",
    "    system_prompt=(\n",
    "        \"You are the Planner. Given origin/destination city, dates (YYYY-MM-DD), budget and themes, \"\n",
    "        \"produce a concise skeleton itinerary (3‚Äì10 days). \"\n",
    "        \"Return PlanSkeleton {destination_city,start_date,end_date,day_by_day[]} with valid YYYY-MM-DD dates only.\"\n",
    "    ),\n",
    "    output_type=PlanSkeleton,\n",
    "    retries=2, output_retries=2,\n",
    ")\n",
    "\n",
    "RESEARCHER = Agent[None, ResearchOutput](\n",
    "    model=MODEL,\n",
    "    mcp_servers=servers,\n",
    "    system_prompt=(\n",
    "        \"You are the Researcher. For each day/date in the given city and themes, \"\n",
    "        \"use web_search and fetch to find 2‚Äì3 specific things to do (landmark, cafe, hike). \"\n",
    "        \"Return ResearchOutput with days[].ideas[] as Links(title,url,notes). Keep items relevant to the city.\"\n",
    "    ),\n",
    "    output_type=ResearchOutput,\n",
    "    retries=2, output_retries=2,\n",
    ")\n",
    "\n",
    "# 8) Weather helpers\n",
    "FORECAST_MAX_DAYS = 16\n",
    "\n",
    "async def wx_forecast_exact(lat: float, lon: float, start_ymd: str, end_ymd: str, tz: str = \"auto\") -> List[Dict[str, Any]]:\n",
    "    import httpx\n",
    "    params = {\n",
    "        \"latitude\": lat, \"longitude\": lon,\n",
    "        \"daily\": [\"temperature_2m_max\",\"temperature_2m_min\",\"precipitation_sum\"],\n",
    "        \"timezone\": tz, \"start_date\": start_ymd, \"end_date\": end_ymd,\n",
    "    }\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=25) as client:\n",
    "            d = (await client.get(\"https://api.open-meteo.com/v1/forecast\", params=params)).json().get(\"daily\", {})\n",
    "    except Exception:\n",
    "        return []\n",
    "    times = d.get(\"time\") or []; tmax = d.get(\"temperature_2m_max\") or []; tmin = d.get(\"temperature_2m_min\") or []; pmm = d.get(\"precipitation_sum\") or []\n",
    "    out = []\n",
    "    for i, day in enumerate(times):\n",
    "        try: out.append({\"date\": day, \"tmax\": float(tmax[i]), \"tmin\": float(tmin[i]), \"precip_mm\": float(pmm[i])})\n",
    "        except Exception: pass\n",
    "    return out\n",
    "\n",
    "async def wx_forecast_window(lat: float, lon: float, need_days_from_today: int, tz: str = \"auto\") -> List[Dict[str, Any]]:\n",
    "    import httpx\n",
    "    params = {\n",
    "        \"latitude\": lat, \"longitude\": lon,\n",
    "        \"daily\": [\"temperature_2m_max\",\"temperature_2m_min\",\"precipitation_sum\"],\n",
    "        \"timezone\": tz, \"forecast_days\": min(max(1, need_days_from_today), FORECAST_MAX_DAYS),\n",
    "    }\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=25) as client:\n",
    "            d = (await client.get(\"https://api.open-meteo.com/v1/forecast\", params=params)).json().get(\"daily\", {})\n",
    "    except Exception:\n",
    "        return []\n",
    "    times = d.get(\"time\") or []; tmax = d.get(\"temperature_2m_max\") or []; tmin = d.get(\"temperature_2m_min\") or []; pmm = d.get(\"precipitation_sum\") or []\n",
    "    out = []\n",
    "    for i, day in enumerate(times):\n",
    "        try: out.append({\"date\": day, \"tmax\": float(tmax[i]), \"tmin\": float(tmin[i]), \"precip_mm\": float(pmm[i])})\n",
    "        except Exception: pass\n",
    "    return out\n",
    "\n",
    "async def wx_era5_last_year(lat: float, lon: float, start_ymd: str, end_ymd: str, tz: str = \"auto\") -> List[Dict[str, Any]]:\n",
    "    import httpx\n",
    "    ly_start = (datetime.strptime(start_ymd, \"%Y-%m-%d\") - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "    ly_end   = (datetime.strptime(end_ymd,   \"%Y-%m-%d\") - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "    params = {\n",
    "        \"latitude\": lat, \"longitude\": lon,\n",
    "        \"daily\": [\"temperature_2m_max\",\"temperature_2m_min\",\"precipitation_sum\"],\n",
    "        \"timezone\": tz, \"start_date\": ly_start, \"end_date\": ly_end,\n",
    "    }\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=25) as client:\n",
    "            d = (await client.get(\"https://archive-api.open-meteo.com/v1/era5\", params=params)).json().get(\"daily\", {})\n",
    "    except Exception:\n",
    "        return []\n",
    "    times = d.get(\"time\") or []; tmax = d.get(\"temperature_2m_max\") or []; tmin = d.get(\"temperature_2m_min\") or []; pmm = d.get(\"precipitation_sum\") or []\n",
    "    out = []\n",
    "    for i, day in enumerate(times):\n",
    "        try: out.append({\"date\": day, \"tmax\": float(tmax[i]), \"tmin\": float(tmin[i]), \"precip_mm\": float(pmm[i])})\n",
    "        except Exception: pass\n",
    "    return out\n",
    "\n",
    "def wx_tip(tmin: float, tmax: float, pmm: float) -> str:\n",
    "    tip = \"Light layers.\"\n",
    "    if tmin < 12: tip = \"Pack a light jacket.\"\n",
    "    if pmm >= 3: tip += \" Chance of rain ‚Äî bring an umbrella.\"\n",
    "    return f\"Wx: {tmin:.0f}‚Äì{tmax:.0f}¬∞C, {pmm:.0f} mm. {tip}\"\n",
    "\n",
    "async def build_weather_map(lat: float, lon: float, start_ymd: str, end_ymd: str) -> Dict[str, Dict[str, float]]:\n",
    "    dates = daterange(start_ymd, end_ymd)\n",
    "    wx = await wx_forecast_exact(lat, lon, start_ymd, end_ymd)\n",
    "    wx_map = {r[\"date\"]: r for r in wx}\n",
    "    missing = [d for d in dates if d not in wx_map]\n",
    "    if missing:\n",
    "        today = datetime.utcnow().date()\n",
    "        days_ahead = (datetime.strptime(start_ymd, \"%Y-%m-%d\").date() - today).days\n",
    "        need_days = max(0, days_ahead) + len(dates)\n",
    "        if days_ahead <= FORECAST_MAX_DAYS:\n",
    "            window = await wx_forecast_window(lat, lon, need_days)\n",
    "            for r in window:\n",
    "                if r[\"date\"] in dates:\n",
    "                    wx_map[r[\"date\"]] = r\n",
    "            missing = [d for d in dates if d not in wx_map]\n",
    "    if missing:\n",
    "        ly = await wx_era5_last_year(lat, lon, start_ymd, end_ymd)\n",
    "        ly_map = {r[\"date\"]: r for r in ly}\n",
    "        for d in missing:\n",
    "            if d in ly_map: wx_map[d] = ly_map[d]\n",
    "        missing = [d for d in dates if d not in wx_map]\n",
    "    for d in missing:\n",
    "        wx_map[d] = {\"date\": d, \"tmax\": 24.0, \"tmin\": 18.0, \"precip_mm\": 1.0}\n",
    "    return wx_map\n",
    "\n",
    "# 9) Deterministic cost model\n",
    "def estimate_costs(distance_km: float, nights: int, days: int, budget_band: str = \"economy\") -> CostSummary:\n",
    "    if distance_km <= 0:\n",
    "        cpm = 0.10\n",
    "    elif distance_km > 6000:\n",
    "        cpm = 0.12\n",
    "    elif distance_km > 3000:\n",
    "        cpm = 0.10\n",
    "    else:\n",
    "        cpm = 0.07\n",
    "    flights = distance_km * cpm\n",
    "\n",
    "    band = (budget_band or \"economy\").lower()\n",
    "    nightly = 140 if band == \"economy\" else (200 if band == \"mid\" else 280)\n",
    "    hotels = nights * nightly\n",
    "\n",
    "    act_day = 35 if band == \"economy\" else (50 if band == \"mid\" else 80)\n",
    "    trn_day = 12 if band == \"economy\" else (18 if band == \"mid\" else 25)\n",
    "    activities = days * act_day\n",
    "    transit    = days * trn_day\n",
    "\n",
    "    total = flights + hotels + activities + transit\n",
    "    return CostSummary(currency=\"USD\",\n",
    "                       flights=round(flights, 2),\n",
    "                       hotels=round(hotels, 2),\n",
    "                       activities=round(activities, 2),\n",
    "                       transit=round(transit, 2),\n",
    "                       total=round(total, 2))\n",
    "\n",
    "# 10) Orchestrator\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "async def run_agent(agent: Agent, prompt: str):\n",
    "    async with agent.run_stream(prompt) as stream:\n",
    "        return await stream.get_output()\n",
    "\n",
    "def _relevant_to_city(link: Link, city: str) -> bool:\n",
    "    blob = f\"{link.title or ''} {link.notes or ''} {link.url or ''}\".lower()\n",
    "    return city.lower() in blob\n",
    "\n",
    "async def multi_agent_trip(origin_input: str, dest_input: str, start_date: str, end_date: str, budget: str, themes: str) -> TripPlan:\n",
    "    # Resolve places\n",
    "    origin_name, origin_geo = await resolve_place(origin_input)\n",
    "    dest_name, dest_geo     = await resolve_place(dest_input)\n",
    "\n",
    "    # 1) Planner\n",
    "    planner_prompt = (\n",
    "        f\"Origin: {origin_name}\\n\"\n",
    "        f\"Destination: {dest_name}\\n\"\n",
    "        f\"Dates: {start_date} to {end_date}\\n\"\n",
    "        f\"Budget: {budget}\\n\"\n",
    "        f\"Themes: {themes}\\n\"\n",
    "        \"Return a PlanSkeleton.\"\n",
    "    )\n",
    "    print(\"üß≠ Planner‚Ä¶\")\n",
    "    skel: PlanSkeleton = await run_agent(PLANNER, planner_prompt)\n",
    "\n",
    "    # Dates: sanitize + fill all days\n",
    "    safe_start = skel.start_date if _valid_ymd(skel.start_date) else start_date\n",
    "    safe_end   = skel.end_date   if _valid_ymd(skel.end_date)   else end_date\n",
    "    dates = daterange(safe_start, safe_end)  # inclusive\n",
    "    day_set = set(d.date for d in (skel.day_by_day or []) if _valid_ymd(d.date))\n",
    "    for d in dates:\n",
    "        if d not in day_set:\n",
    "            (skel.day_by_day).append(DayPlan(date=d, summary=f\"{dest_name} highlights\"))\n",
    "    skel.day_by_day = sorted(skel.day_by_day, key=lambda x: x.date)\n",
    "\n",
    "    # 2) Weather\n",
    "    print(\"‚õÖ Weather‚Ä¶\")\n",
    "    if dest_geo:\n",
    "        wx_map = await build_weather_map(dest_geo[\"lat\"], dest_geo[\"lon\"], safe_start, safe_end)\n",
    "    else:\n",
    "        wx_map = {d: {\"date\": d, \"tmax\": 24.0, \"tmin\": 18.0, \"precip_mm\": 1.0} for d in dates}\n",
    "\n",
    "    # 3) Research (no fallback ideas; keep only city-relevant items)\n",
    "    print(\"üîé Research‚Ä¶\")\n",
    "    research_prompt = (\n",
    "        f\"City: {dest_name}\\nDates: {', '.join(dates)}\\nThemes: {themes}\\n\"\n",
    "        \"Return ResearchOutput with 2‚Äì3 ideas per date (Links with title,url,notes).\"\n",
    "    )\n",
    "    research: ResearchOutput = await run_agent(RESEARCHER, research_prompt)\n",
    "    research.days = [d for d in (research.days or []) if _valid_ymd(d.date)]\n",
    "    ideas_map: Dict[str, List[Link]] = {}\n",
    "    for d in research.days:\n",
    "        filtered = [lnk for lnk in (d.ideas or []) if _relevant_to_city(lnk, dest_name)]\n",
    "        ideas_map[d.date] = filtered[:3]\n",
    "\n",
    "    # 4) Deterministic costs\n",
    "    def haversine_km(lat1, lon1, lat2, lon2):\n",
    "        if None in (lat1, lon1, lat2, lon2): return 0.0\n",
    "        R = 6371.0\n",
    "        p1, p2 = math.radians(lat1), math.radians(lat2)\n",
    "        dphi = math.radians(lat2 - lat1)\n",
    "        dlmb = math.radians(lon2 - lon1)\n",
    "        a = math.sin(dphi/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlmb/2)**2\n",
    "        return 2 * R * math.asin(math.sqrt(a))\n",
    "    lat1 = origin_geo[\"lat\"] if origin_geo else None\n",
    "    lon1 = origin_geo[\"lon\"] if origin_geo else None\n",
    "    lat2 = dest_geo[\"lat\"]   if dest_geo   else None\n",
    "    lon2 = dest_geo[\"lon\"]   if dest_geo   else None\n",
    "    distance_km = haversine_km(lat1, lon1, lat2, lon2)\n",
    "    nights = max(0, (datetime.strptime(safe_end, \"%Y-%m-%d\") - datetime.strptime(safe_start, \"%Y-%m-%d\")).days)\n",
    "    days = len(dates)\n",
    "    costs = estimate_costs(distance_km, nights, days, budget)\n",
    "\n",
    "    # 5) Links (Flights + Booking.com only) with computed prices\n",
    "    per_night = (costs.hotels / max(1, nights)) if nights else None\n",
    "    flights = [Link(\n",
    "        title=\"Google Flights\",\n",
    "        url=google_flights_link(origin_name, dest_name, safe_start, safe_end),\n",
    "        price=round(costs.flights, 0),\n",
    "        notes=\"Estimated roundtrip (distance √ó CPM); click for live fares\",\n",
    "    )]\n",
    "    hotels = [Link(\n",
    "        title=\"Booking.com\",\n",
    "        url=booking_link(dest_name, safe_start, safe_end),\n",
    "        price=round(costs.hotels, 0),\n",
    "        notes=f\"Estimated total for {nights} night(s)\" + (f\" (‚âà ${per_night:,.0f}/night)\" if per_night else \"\"),\n",
    "    )]\n",
    "\n",
    "    # 6) Merge day plans + weather tip\n",
    "    merged_days: List[DayPlan] = []\n",
    "    for d in skel.day_by_day:\n",
    "        if not _valid_ymd(d.date): continue\n",
    "        acts = list(d.activities or [])\n",
    "        for link in ideas_map.get(d.date, []):\n",
    "            acts.append(f\"{link.title} ‚Äî {link.url}\" if link.url else link.title)\n",
    "        w = wx_map.get(d.date)\n",
    "        if w:\n",
    "            acts.append(wx_tip(w[\"tmin\"], w[\"tmax\"], w[\"precip_mm\"]))\n",
    "        merged_days.append(DayPlan(date=d.date, summary=d.summary or f\"{dest_name} highlights\", activities=acts))\n",
    "\n",
    "    # NOTE: Fixed Local transit text per request (always Taipei wording)\n",
    "    fixed_transit_text = \"Consider local transit passes in Taipei. Compare airport rail vs rideshare.\"\n",
    "\n",
    "    return TripPlan(\n",
    "        flights=flights,\n",
    "        hotels=hotels,\n",
    "        day_by_day=merged_days,\n",
    "        transit_notes=fixed_transit_text,\n",
    "        links=[],\n",
    "        cost_summary=costs,\n",
    "    )\n",
    "\n",
    "def render_plan(plan: TripPlan) -> None:\n",
    "    def _links_table(items: List[Link], label: str) -> str:\n",
    "        if not items: return f\"_No {label.lower()} returned._\"\n",
    "        rows = [\"| # | Title | Price | Notes |\", \"|---:|---|---:|---|\"]\n",
    "        for i, l in enumerate(items, 1):\n",
    "            title = f\"[{l.title}]({l.url})\" if l.url else (l.title or \"\")\n",
    "            price = f\"${l.price:,.0f}\" if (l.price is not None) else \"\"\n",
    "            notes = l.notes or \"\"\n",
    "            rows.append(f\"| {i} | {title} | {price} | {notes} |\")\n",
    "        return \"\\n\".join(rows)\n",
    "\n",
    "    def _day_table(days: List[DayPlan]) -> str:\n",
    "        if not days: return \"_No day-by-day plan returned._\"\n",
    "        rows = [\"| Date | Plan |\", \"|---|---|\"]\n",
    "        for d in days:\n",
    "            bullets = \"<br>\".join(f\"‚Ä¢ {a}\" for a in (d.activities or []))\n",
    "            plan = (d.summary or \"\") + ((\"<br>\" + bullets) if bullets else \"\")\n",
    "            rows.append(f\"| {d.date} | {plan} |\")\n",
    "        return \"\\n\".join(rows)\n",
    "\n",
    "    parts = []\n",
    "    parts += [\"## ‚úàÔ∏è Flights\", _links_table(plan.flights, \"Flight links\")]\n",
    "    parts += [\"\\n\\n## üè® Hotels\", _links_table(plan.hotels, \"Hotel links\")]\n",
    "    parts += [\"\\n\\n## üìÖ Day-by-day\", _day_table(plan.day_by_day)]\n",
    "    parts += [\"\\n\\n## üöâ Local transit\", plan.transit_notes or \"_‚Äî_\"]\n",
    "    if plan.links:\n",
    "        parts += [\"\\n\\n## üîó Extra links\", _links_table(plan.links, \"Links\")]\n",
    "    cs = plan.cost_summary\n",
    "    parts += [\n",
    "        \"\\n\\n## üíµ Cost summary\",\n",
    "        f\"\"\"| Item | Cost ({cs.currency}) |\n",
    "|---|---:|\n",
    "| Flights | {cs.flights:,.0f} |\n",
    "| Hotels | {cs.hotels:,.0f} |\n",
    "| Activities | {cs.activities:,.0f} |\n",
    "| Transit | {cs.transit:,.0f} |\n",
    "| **Total** | **{cs.total:,.0f}** |\"\"\"\n",
    "    ]\n",
    "    display(Markdown(\"\\n\".join(parts)))\n",
    "\n",
    "# 11) Demo helper\n",
    "async def run_demo(\n",
    "    origin_airport: str = \"SFO\",\n",
    "    dest_city: str = \"NRT\",\n",
    "    start_date: str = \"2025-08-22\",\n",
    "    end_date: str = \"2025-08-31\",\n",
    "    budget: str = \"economy\",\n",
    "    themes: str = \"Manga, sushi, japanese culture, and historical buildings\",\n",
    "):\n",
    "    plan = await multi_agent_trip(origin_airport, dest_city, start_date, end_date, budget, themes)\n",
    "    render_plan(plan)\n",
    "\n",
    "print(\"üöÄ Running demo...\")\n",
    "asyncio.run(run_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step7\"></a>\n",
    "\n",
    "## Step 7: Challenge - Expand the Agent\n",
    "\n",
    "**Task:** The challenge for this workshop will be announced during the workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß≠ Planner‚Ä¶\n",
      "[2025-08-15 19:06:01] INFO:     127.0.0.1:43420 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-15 19:06:01] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 396, token usage: 0.02, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 19:06:01] Decode batch. #running-req: 1, #token: 434, token usage: 0.02, cuda graph: True, gen throughput (token/s): 0.75, #queue-req: 0, \n",
      "[2025-08-15 19:06:01] Decode batch. #running-req: 1, #token: 474, token usage: 0.02, cuda graph: True, gen throughput (token/s): 186.71, #queue-req: 0, \n",
      "[2025-08-15 19:06:01] Decode batch. #running-req: 1, #token: 514, token usage: 0.03, cuda graph: True, gen throughput (token/s): 186.42, #queue-req: 0, \n",
      "[2025-08-15 19:06:01] Decode batch. #running-req: 1, #token: 554, token usage: 0.03, cuda graph: True, gen throughput (token/s): 186.21, #queue-req: 0, \n",
      "[2025-08-15 19:06:02] Decode batch. #running-req: 1, #token: 594, token usage: 0.03, cuda graph: True, gen throughput (token/s): 186.06, #queue-req: 0, \n",
      "[2025-08-15 19:06:02] Decode batch. #running-req: 1, #token: 634, token usage: 0.03, cuda graph: True, gen throughput (token/s): 185.78, #queue-req: 0, \n",
      "[2025-08-15 19:06:02] Decode batch. #running-req: 1, #token: 674, token usage: 0.03, cuda graph: True, gen throughput (token/s): 185.84, #queue-req: 0, \n",
      "[2025-08-15 19:06:02] Decode batch. #running-req: 1, #token: 714, token usage: 0.03, cuda graph: True, gen throughput (token/s): 185.71, #queue-req: 0, \n",
      "[2025-08-15 19:06:02] Decode batch. #running-req: 1, #token: 754, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.49, #queue-req: 0, \n",
      "[2025-08-15 19:06:03] Decode batch. #running-req: 1, #token: 794, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.55, #queue-req: 0, \n",
      "[2025-08-15 19:06:03] Decode batch. #running-req: 1, #token: 834, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.44, #queue-req: 0, \n",
      "[2025-08-15 19:06:03] Decode batch. #running-req: 1, #token: 874, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.41, #queue-req: 0, \n",
      "[2025-08-15 19:06:03] Decode batch. #running-req: 1, #token: 914, token usage: 0.04, cuda graph: True, gen throughput (token/s): 185.32, #queue-req: 0, \n",
      "‚õÖ Weather‚Ä¶\n",
      "üîé Research‚Ä¶\n",
      "[2025-08-15 19:06:04] INFO:     127.0.0.1:43420 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-15 19:06:04] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 436, token usage: 0.02, #running-req: 0, #queue-req: 0, \n",
      "[2025-08-15 19:06:04] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.71, #queue-req: 0, \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## ‚úàÔ∏è Flights\n",
       "| # | Title | Price | Notes |\n",
       "|---:|---|---:|---|\n",
       "| 1 | [Google Flights](https://www.google.com/travel/flights/search?q=Flights%20to%20Seoul%20from%20San%20Francisco%20on%202025-08-22%20through%202025-08-02) | $1,083 | Estimated roundtrip (distance √ó CPM); click for live fares |\n",
       "\n",
       "\n",
       "## üè® Hotels\n",
       "| # | Title | Price | Notes |\n",
       "|---:|---|---:|---|\n",
       "| 1 | [Booking.com](https://www.booking.com/searchresults.html?ss=Seoul&checkin=2025-08-22&checkout=2025-08-02) | $0 | Estimated total for 0 night(s) |\n",
       "\n",
       "\n",
       "## üìÖ Day-by-day\n",
       "| Date | Plan |\n",
       "|---|---|\n",
       "| 2025-08-22 | Welcome to Seoul, the vibrant heart of South Korea. Start your day with a visit to Gyeongbokgung Palace, a symbol of Korean history.<br>‚Ä¢ Visit Gyeongbokgung Palace<br>‚Ä¢ Explore Namsangol Hanok Village |\n",
       "| 2025-08-23 | Today, we explore Seoul's cultural side. We'll visit the National Museum of Korea and The Seoul Museum of Art.<br>‚Ä¢ Visit National Museum of Korea<br>‚Ä¢ Explore The Seoul Museum of Art |\n",
       "| 2025-08-24 | Sample traditional Korean cuisine at Myeong-dong. In the evening, enjoy a meal at a local restaurant in Bukchon Hanok Village.<br>‚Ä¢ Lunch: Myeong-dong Street<br>‚Ä¢ Dinner: Bukchon Hanok Village |\n",
       "| 2025-08-25 | Discover the Chinese influence in Seoul with a visit to Manse Church and Chongmyo Royal Ancestral Shrine.<br>‚Ä¢ Visit Manse Church<br>‚Ä¢ Explore Chongmyo Royal Ancestral Shrine |\n",
       "| 2025-08-26 | Explore the trendy neighborhoods of Hongdae and Itaewon. Enjoy a night out at Itaewon for shopping and dining.<br>‚Ä¢ Visit Hongdae and Its Art Galleries<br>‚Ä¢ Dine at Itaewon |\n",
       "| 2025-08-27 | Day off to relax. Enjoy a spa or visit the nearby Insa-dong for shopping and tea houses.<br>‚Ä¢ Relax at a Spa<br>‚Ä¢ Visit Insa-dong |\n",
       "| 2025-08-28 | Visit the modern side of Seoul with a trip to Lotte World Tower and visit the futuristic Dongdaemun Design Plaza.<br>‚Ä¢ Visit Lotte World Tower<br>‚Ä¢ Explore Dongdaemun Design Plaza |\n",
       "| 2025-08-29 | Last day in Seoul! Visit the famous Myeong-dong shopping district and grab a last meal at a local restaurant.<br>‚Ä¢ Shopping and sightseeing in Myeong-dong<br>‚Ä¢ Dinner: Local Restaurant |\n",
       "\n",
       "\n",
       "## üöâ Local transit\n",
       "Consider local transit passes in Taipei. Compare airport rail vs rideshare.\n",
       "\n",
       "\n",
       "## üíµ Cost summary\n",
       "| Item | Cost (USD) |\n",
       "|---|---:|\n",
       "| Flights | 1,083 |\n",
       "| Hotels | 0 |\n",
       "| Activities | 0 |\n",
       "| Transit | 0 |\n",
       "| **Total** | **1,083** |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: uncomment any to try\n",
    "# asyncio.run(run_demo(\"SFO\", \"China\", \"2025-08-22\", \"2025-08-31\", \"economy\", \"Food, Chinese culture, and historical buildings\"))\n",
    "asyncio.run(run_demo(\"SFO\", \"Seoul\", \"2025-08-22\", \"2025-07-02\", \"economy\", \"Food, Chinese culture, and historical buildings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy coding! If you encounter issues or have questions, don‚Äôt hesitate to ask or raise an issue on our [Github page](https://github.com/ROCm/gpuaidev)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
