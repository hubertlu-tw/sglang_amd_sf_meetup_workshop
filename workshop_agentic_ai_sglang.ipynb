{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your AI Agent with MCPs using SGLang, Pydantic AI, and AMD MI300X GPU\n",
    "\n",
    "Welcome to this hands-on workshop! In this tutorial, we'll leverage AMD GPUs and **Model Context Protocol (MCP)** ,an open standard for exposing LLM tools via API, to deploy powerful language models like Qwen3-30B. Key components:\n",
    "- üñ•Ô∏è **SGLang** for GPU-optimized inference\n",
    "- üõ†Ô∏è **Pydantic-AI** for agent/tool management\n",
    "- üîå **MCP Servers** for pre-built tool integration\n",
    "\n",
    "You'll learn how to set up your environment, deploy large language models like Qwen3-30B, connect them to real-world tools using MCP, and build a conversational agent capable of reasoning and taking actions\n",
    "\n",
    "By the end of this workshop, you‚Äôll have built an AI-powered assistant agent that can find a place to stay based on your preferences like location, budget, and travel dates.\n",
    "\n",
    "Let‚Äôs dive in!\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Step 1: Launching SGLang Server on AMD GPUs](#step1)\n",
    "- [Step 2: Installing Dependencies](#step2)\n",
    "- [Step 3: Create a simple instance of Pydantic-AI Agent](#step3)\n",
    "- [Step 4: Write a Date/Time Tool for your Agent](#step4)\n",
    "- [Step 5: Replace your Date/Time Tool with a MCP Server](#step5)\n",
    "- [Step 6: Turn your Agent into a trip planner](#step6)\n",
    "- [Step 7: Challenge](#step7)\n",
    "\n",
    "<a id=\"step1\"></a>\n",
    "\n",
    "## Step 1: Launch SGLang Server on AMD GPUs\n",
    "\n",
    "In this workshop, you are going to use [SGLang](https://github.com/sgl-project/sglang) inference serving engine. SGLang provides many benefits such as fast model execution, extensive list of supported models, easy to use, and best of all it's open-source. \n",
    "\n",
    "### Deploy Qwen3-30B Model with SGLang (~2mins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to start a SGLang server and create an OpenAI-compatible endpoint for your LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sglang.utils import launch_server_cmd\n",
    "os.environ[\"SGLANG_USE_AITER\"] = \"1\"\n",
    "server_process, port = launch_server_cmd(\n",
    "    \"python3 -m sglang.launch_server --served-model-name Qwen/Qwen3-30B-A3B-Instruct-2507 --model-path /models/Qwen3-30B-A3B-Instruct-2507 --tool-call-parser qwen25 --host 0.0.0.0\"\n",
    ")\n",
    "\n",
    "BASE_URL = f\"http://localhost:{port}/v1\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"BASE_URL\"]    = BASE_URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"abc-123\"   \n",
    "\n",
    "print(\"Config set:\", BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Before continuing, wait until you see \"The server is fired up and ready to roll!\"***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server is ready, you can verify the model is available with a curl command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl $BASE_URL/models -H \"Authorization: Bearer $OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you just launched a powerful server that can serve any incoming request and allowing you to build amazing applications. Wasn't that easy?üéâ \n",
    "\n",
    "<a id=\"step2\"></a>\n",
    "\n",
    "## Step 2: Installing Dependencies\n",
    "\n",
    "Install the dependencies for this notebook using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pydantic_ai openai  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"step3\"></a>\n",
    "\n",
    "## Step 3: Create a simple instance of Pydantic-AI Agent\n",
    "\n",
    "Start by creating a custom OpenAI Compatible endpoint for your agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "\n",
    "provider = OpenAIProvider(\n",
    "    base_url=os.environ[\"BASE_URL\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "agent_model = OpenAIModel(\"Qwen/Qwen3-30B-A3B-Instruct-2507\", provider=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of the `Agent` class from `pydantic_ai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=agent_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to test the agent. `pydantic_ai` provides multiple ways to run `Agent`. You can learn more about it from [the PydanticAI site](https://ai.pydantic.dev/agents/#running-agents).\n",
    "\n",
    "For this workshop, you'll run the agent in async mode to make testing much faster. Let's define the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_async(prompt: str) -> str:\n",
    "    async with agent:\n",
    "        result = await agent.run(prompt)\n",
    "        return result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test the agent by calling this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_async(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "\n",
    "## Step 4: Write a Date/Time Tool for Your Agent\n",
    "\n",
    "LLMs rely on their training data to respond to your prompts, so the agent you just defined will fail to answer factual questions that fall outside of its training knowledge. You can see this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is no surprise that the model failed to answer this question. Now, it's time to power-up your LLM by providing `agent` a function that can get the current date. The process of an LLM triggering a function call is commonly referred to as `Tool Calling` or `Function Calling`. In this workshop we are going to take advantage of `pydantic-ai`'s agent `tools` to provide your agent with appropriate tools. First, define a custom tool within this framework using the code sample below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pydantic_ai import Tool          \n",
    "@Tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Return the current date/time as an ISO-formatted string.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, provide this tool to your agent by adding the function signature of the tool to the `Agent` constructor. This notifies the LLM that the new tool exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    tools=[get_current_date],\n",
    "    system_prompt = (\n",
    "        \"You have access to:\\n\"\n",
    "        \"   1. get_current_time(params: dict)\\n\"\n",
    "        \"Use this tool for date/time questions.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building an agent with access to real-time data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"step5\"></a>\n",
    "\n",
    "## Step 5: Replace Your Date/time Tool with a MCP Server\n",
    "\n",
    "Now that you learned how to create a custom tool and let the agent access it, you can enhance this step using [Model Context Protocol](https://modelcontextprotocol.io/introduction). You can replace the custom tool with a simple MCP server that serves the agent and provides similar information.\n",
    "\n",
    "**Why MCP?** MCP servers provide:\n",
    "- ‚úÖ Standardized API interfaces\n",
    "- üîÑ Reusable across projects\n",
    "- üì¶ Pre-built functionality\n",
    "\n",
    "Let's replace your custom time tool with an official MCP time server.\n",
    "\n",
    "### Installing Time MCP Server\n",
    "\n",
    "Start by installing a [Time/Date MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/time):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mcp-server-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the time_server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "\n",
    "time_server = MCPServerStdio(\n",
    "    \"python\",\n",
    "    args=[\n",
    "        \"-m\", \"mcp_server_time\",\n",
    "        \"--local-timezone=America/New_York\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, modify your agent by removing the previously defined tool and adding the MCP server instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    toolsets=[time_server],\n",
    "    system_prompt = (\n",
    "        \"You are a helpful agent and you have access to this tool:\\n\"\n",
    "        \"   get_current_time(params: dict)\\n\"\n",
    "        \"When the user asks for the current date or time, call get_current_time.\\n\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Great, you can now see whether the agent can use MCP to provide the correct time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_async(\"What‚Äôs the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tadaa! Now you have officially used MCP servers to power-up your AI agents. In the next section we show how you can your turn many ideas into real working projects by using free MCP servers available today.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"step6\"></a>\n",
    "\n",
    "## Step 6: Turn your agent into a travel planner\n",
    "\n",
    "As you saw in the last section, MCP servers are really easy to use and they provide a standard way of providing LLMs the tools they need. There are already thousands of MCP servers available to use and MCP trackers that share available servers. Here are some for your reference:\n",
    "- https://github.com/modelcontextprotocol/servers\n",
    "- https://mcp.so/\n",
    "\n",
    "You are going to use npx to launch out the next server. Let's install the required dependencies first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Node.js 20 via NodeSource\n",
    "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\n",
    "!apt install -y nodejs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify `npm` and `npx` installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!node -v && npm -v && npx --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the workshop we are going to build an agent that can help you browse available Airbnbs to book. We can now build on top of what we have so far and add an open-source [Airbnb MCP server](https://github.com/openbnb-org/mcp-server-airbnb) to our agent. To do so, let's start by defining our Airbnb server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_server = MCPServerStdio(\n",
    "    \"npx\", args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You have access to three tools:\n",
    "1. get_current_time(params: dict)\n",
    "2. airbnb_search(params: dict)\n",
    "3. airbnb_listing_details(params: dict)\n",
    "When the user asks for listings, first call get_current_time, then airbnb_search, etc.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=agent_model,\n",
    "    toolsets=[time_server, airbnb_server],\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try our agent and see if it can browse through Airbnb listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_async(\"Find a place to stay in San Francisco for next Sunday for 3 nights for 2 adults?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy coding! If you encounter issues or have questions, don‚Äôt hesitate to ask or raise an issue on our [Github page](https://github.com/ROCm/gpuaidev)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step7\"></a>\n",
    "\n",
    "## Step 7: Challenge - Expand the Agent\n",
    "\n",
    "**Task:** The challenge for this workshop will be announced during the workshop. \n",
    "\n",
    "You can open a terminal and watch the GPU utilization by running this command:\n",
    "\n",
    "```bash \n",
    "watch rocm-smi\n",
    "```\n",
    "\n",
    "Let's set some environment variables for our server to use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the process of SGLang server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang.utils import terminate_process\n",
    "terminate_process(server_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
